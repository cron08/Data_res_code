{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPW1ZnKxnkua"
      },
      "outputs": [],
      "source": [
        "#@title Your Info { display-mode: \"form\" }\n",
        "\n",
        "Name = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataRes Research Coding Assessment\n",
        "\n",
        "## Due by 11:59pm, October 10\n",
        "\n",
        "Thanks for taking the time to apply to the research team at DataRes! This assessment consists of handful of machine learning exercises that use PyTorch. If you aren't familiar with this library, there's lots of helpful documentation [here](https://pytorch.org/docs/stable/index.html). Lots of code has already been written for you, just make sure to complete sections marked \"TODO:\" or \"Your Implementations\". There are a number of challenging problems to work on, so if you have any questions, please feel free to reach out to [Lukas](mailto:lukasbroc@gmail.com) or [Sammy](mailto:sammy7shang@gmail.com) at any time! Lastly, please feel free to submit the assessment even if you can't finish it all, we will take the time to look at every application we recieve.\n",
        "\n",
        "Make sure to submit this completed assessment by 11:59pm, October 10. Submissions will be turned in by emailing this notebook as an .ipynb and the .json with your model's results (more on this below) to both of us.\n",
        "\n",
        "Good luck!"
      ],
      "metadata": {
        "id": "TfC9Q01STSE1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbJHvrBLSTp7"
      },
      "source": [
        "### Overview\n",
        "\n",
        "In this assessment, you will be working with the [MiniPlaces dataset](https://github.com/CSAILVision/miniplaces), a dataset of scene images (10+ million images) with a wide variety of real-world environments (400+ unique scene categories). The MiniPlaces dataset is a subset of the [Places2 dataset](http://places2.csail.mit.edu/) and contains 100,000 images for training, 10,000 images for validation, and 10,000 images for testing, each of which has been annotated with one of 100 different scene categories. These images are divided into three folders: train, val, and test.\n",
        "\n",
        "Question 1 will use only some categories of this dataset. We've called this new dataset TinyPlaces. Questions 2-4 will use MiniPlaces (all subcategories).\n",
        "\n",
        "You will be completing machine learning exercises in which you will train models using this dataset. These exercises touch on the following topics:\n",
        "\n",
        "\n",
        "*   Linear, Logistic, and Softmax Regression\n",
        "*   Multi-Layer Perceptrons\n",
        "*   Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Don't worry about having to deal with preprocessing the data... we've already written code to do that for you! Just run the cell below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol_RX3yIBi3w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path = './'\n",
        "os.makedirs(os.path.join(path, 'DataRes_Research_Assessment', 'data'), exist_ok=True)\n",
        "root_dir = os.path.join(path, 'DataRes_Research_Assessment')\n",
        "\n",
        "\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!gdown 1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI\n",
        "\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "total_size = sum(f.size for f in tar.getmembers())\n",
        "with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "    for member in tar.getmembers():\n",
        "        tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "        pbar.update(member.size)\n",
        "tar.close()\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_subcategories(txt_path):\n",
        "    subcategories = {}\n",
        "    file = open(txt_path, 'r')\n",
        "    lines = file.readlines()\n",
        "    for i, l in enumerate(lines):\n",
        "        info = l.split()\n",
        "        info[0] = info[0][3:]\n",
        "        subcategories.update({info[0]: {'ori_class_id': int(info[1]), 'class_id': i}})\n",
        "\n",
        "    return subcategories\n",
        "\n",
        "\n",
        "from tqdm import trange\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def select_samples(subcategories, root_dir, split, n_images_per_subcategory):\n",
        "    samples = []\n",
        "    if split == \"train\":\n",
        "        train_dir = os.path.join(root_dir, \"data\", \"images\", \"train\")\n",
        "        for i in subcategories:\n",
        "            child_dir = os.path.join(train_dir, i[0], i)\n",
        "            pics = random.sample(os.listdir(child_dir), n_images_per_subcategory)\n",
        "            for j in pics:\n",
        "                samples.append((cv2.resize(cv2.imread(os.path.join(child_dir, j)), (32,32)).flatten().tolist(), subcategories[i][\"class_id\"]))\n",
        "    elif split == \"val\":\n",
        "        val_dir = os.path.join(root_dir, \"data\", \"images\")\n",
        "        file = open(os.path.join(root_dir, \"data\", \"val.txt\"), 'r')\n",
        "        lines = file.readlines()\n",
        "        val_data = []\n",
        "        for i in lines:\n",
        "            val_data.append(i.split())\n",
        "        random.shuffle(val_data)\n",
        "        for i in subcategories:\n",
        "            old_id = subcategories[i][\"ori_class_id\"]\n",
        "            count = 0\n",
        "            for j in val_data:\n",
        "                if int(j[1]) == old_id:\n",
        "                    samples.append((cv2.resize(cv2.imread(os.path.join(val_dir, j[0])), (32,32)).flatten().tolist(), subcategories[i][\"class_id\"]))\n",
        "                    count += 1\n",
        "                if count >= n_images_per_subcategory:\n",
        "                    break\n",
        "    return samples\n",
        "\n",
        "def create_tinyplaces(samples, binary=True):\n",
        "    data, labels = [], []\n",
        "    for i in samples:\n",
        "        data.append(i[0])\n",
        "        if binary:\n",
        "            if i[1] >= 10:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "        else:\n",
        "            labels.append(i[1])\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "    dataset = {\"data\": data, \"label\": labels}\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Set the root directory of the dataset\n",
        "root_dir = './DataRes_Research_Assessment/data'\n",
        "\n",
        "# Load the target subcategories and their class IDs\n",
        "subcategories = load_subcategories(os.path.join(root_dir, 'data', 'categories_tinyplaces.txt'))\n",
        "\n",
        "# Select the samples from the train split of the TinyPlaces dataset\n",
        "train_samples = select_samples(subcategories, root_dir, 'train', 500)\n",
        "\n",
        "# Create the TinyPlaces datasets for binary and multiclass classification\n",
        "tinyplaces_binary_train = create_tinyplaces(train_samples, binary=True)\n",
        "tinyplaces_multi_train = create_tinyplaces(train_samples, binary=False)\n",
        "\n",
        "# Select the samples from the val split of the MiniPlaces dataset\n",
        "val_samples = select_samples(subcategories, root_dir, 'val', 50)\n",
        "\n",
        "# Create the TinyPlaces datasets for binary and multiclass classification\n",
        "tinyplaces_binary_val = create_tinyplaces(val_samples, binary=True)\n",
        "tinyplaces_multi_val = create_tinyplaces(val_samples, binary=False)\n",
        "\n",
        "# Save the TinyPlaces datasets to the data directory\n",
        "data_dir = os.path.join(root_dir, 'data')\n",
        "\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_binary_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_binary_train, f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_multi_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_multi_train, f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_binary_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_binary_val, f)\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_multi_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_multi_val, f)\n",
        "\n",
        "\n",
        "class TinyPlacesDataset(object):\n",
        "    def __init__(self, data_dict):\n",
        "        self.dataset = data_dict\n",
        "        self.num_samples = len(data_dict['data'])\n",
        "\n",
        "    def subsample(self, ratio=0.1, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        nums = random.sample(range(self.num_samples), int(ratio * self.num_samples))\n",
        "        sub_dataset = {'data': self.dataset['data'][nums], 'label': self.dataset['label'][nums]}\n",
        "        subsampled_dataset = TinyPlacesDataset(sub_dataset)\n",
        "\n",
        "        return subsampled_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfvRB8rdC1RQ"
      },
      "source": [
        "## GPU\n",
        "\n",
        "You will train your models using the GPU. Go to:\n",
        "Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpHEoo1LaZxj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2sV9w1q3Pek"
      },
      "outputs": [],
      "source": [
        "# This line of code gives you info about GPU\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "977hhjjgTmSz"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_binary_train.pkl'), 'rb') as f:\n",
        "    binary_train = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_binary_val.pkl'), 'rb') as f:\n",
        "    binary_val = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_multi_train.pkl'), 'rb') as f:\n",
        "    multi_train = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_multi_val.pkl'), 'rb') as f:\n",
        "    multi_val = TinyPlacesDataset(pickle.load(f))\n",
        "\n",
        "# Convert everything from numpy arrays to tensors and move them to the GPU using .cuda()\n",
        "for dataset in [binary_train, binary_val, multi_train, multi_val]:\n",
        "    for k in ['data', 'label']:\n",
        "        dataset.dataset[k] = torch.tensor(dataset.dataset[k]).float().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "gZmGTp-SD1w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcS_PwFfEwRu"
      },
      "source": [
        "## Q1 Regressions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "RFqPsGNzdwYQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbXT1RrtTyDm"
      },
      "source": [
        "Below, you will implement linear regression. You'll need to use PyTorch to initialize the parameters and implement the linear function and mean squared error.\n",
        "\n",
        "If the prediction score > 0.5, we consider the image to be of outdoor category. Otherwise, we consider it to be indoor category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKbDCvNAEp4w"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(object):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "          Initialize the weights and biases using zeros distribution\n",
        "\n",
        "          Parameters:\n",
        "              input_size (int): The input size (dimension of feature vectors)\n",
        "              output_size (int): The output size (dimension of output logits)\n",
        "\n",
        "          Returns:\n",
        "              None.\n",
        "        \"\"\"\n",
        "        # Initialize the weights and biases using zeros\n",
        "        # Make sure your tensors keep track of their gradient\n",
        "        # Move the parameters to GPU (cuda)\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def linear(self, x):\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To make the output shape compact.\n",
        "        return self.linear(x).squeeze()\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the mean squared error between the predicted labels and the ground-truth labels\n",
        "        loss = None\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def fit(self, x, y, x_val, y_val, lr, epochs=500000, print_freq=1000):\n",
        "        # Fit the linear regression model to the training data using gradient descent\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # lr is the learning rate, epochs is the number of epochs\n",
        "\n",
        "        # To store validation accuracy\n",
        "        val_accs = []\n",
        "        # Create a progress bar using tqdm\n",
        "        pbar = tqdm(range(epochs))\n",
        "        for epoch in pbar:\n",
        "            # Calculate the loss\n",
        "            y_pred_logits = self.forward(x)\n",
        "            loss = self.get_loss(y_pred_logits, y)\n",
        "            # Backpropagate the loss to compute the gradients\n",
        "            loss.backward()\n",
        "            # Update the weights and biases using gradient descent\n",
        "            with torch.no_grad():\n",
        "                self.W -= lr * self.W.grad\n",
        "                self.b -= lr * self.b.grad\n",
        "                # Reset the gradients\n",
        "                self.W.grad.zero_()\n",
        "                self.b.grad.zero_()\n",
        "\n",
        "            if epoch % print_freq == 0:\n",
        "                # Calculate the validation accuracy\n",
        "                val_acc = self.evaluate(x_val, y_val)\n",
        "                val_accs.append(val_acc)\n",
        "                # Update the progress bar with the validation accuracy and training loss\n",
        "                pbar.set_description(f'val_acc: {val_acc:.3f}')\n",
        "        return val_accs\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # Calculate the predicted labels\n",
        "        y_pred = self.forward(x) > 0.5\n",
        "        return (y_pred == y).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrfEfC8HySV5"
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    # We can simply divide x by 255 since its range is (0,255)\n",
        "    return x / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62hOHXjJS4ap"
      },
      "outputs": [],
      "source": [
        "# Here \"ori\" indicates \"original\"\n",
        "X_train_ori, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_val_ori, y_val = binary_val.dataset['data'], binary_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train_ori)\n",
        "X_val = normalize(X_val_ori)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTS_hvlc2pW0"
      },
      "source": [
        "Use the next 3 cells to debug. The next cell should give an accuracy of 0.5 since we're using zeros-initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0A6AHca2-TG"
      },
      "outputs": [],
      "source": [
        "linear_model = LinearRegression(3072, 1)\n",
        "train_acc = linear_model.evaluate(X_train.double(), y_train)\n",
        "val_acc = linear_model.evaluate(X_val.double(), y_val)\n",
        "print('train accuracy:', train_acc)\n",
        "print('val accuracy:', val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLKHdXqC2nL_"
      },
      "outputs": [],
      "source": [
        "# Try to debug using this cell\n",
        "linear_model = LinearRegression(3072, 1)\n",
        "# We refer the raw outputs from a model to as \"logits\",\n",
        "# i.e., we haven't transformed the results to binary labels.\n",
        "y_pred_logits = linear_model.forward(X_train.double())\n",
        "loss = linear_model.get_loss(y_pred_logits, y_train)\n",
        "print(\"loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXz4DNg_7QwK"
      },
      "outputs": [],
      "source": [
        "# compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# check the gradients\n",
        "print(linear_model.W.grad)\n",
        "print(linear_model.b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, you've fully implemented the linear regression model! Let's train it."
      ],
      "metadata": {
        "id": "5zv9nkvydSwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRzp8zPu8gyD"
      },
      "outputs": [],
      "source": [
        "linear_model = LinearRegression(3072, 1)\n",
        "# You can reduce the number of training epochs to debug\n",
        "lin_val_accs = linear_model.fit(X_train.double().cuda(), y_train, X_val.double().cuda(), y_val, 1e-4, 500000, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DGOh6Fm_bMH"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic regression is very similar to linear regression, but the sigmoid function is added to the forward pass and cross-entropy is used for loss. Here's the formulas:\n",
        "\n",
        "Sigmoid:\n",
        "\n",
        "$$\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "Cross-Entropy Loss:\n",
        "\n",
        "$$\\mathrm{CE_Loss}(p, y) = -{(y\\log(p) + (1-y)\\log(1-p))}$$\n",
        "\n",
        "Implement them below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyJW0sfdiMv4"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output\n",
        "\n",
        "\n",
        "def cross_entropy_loss(p, y):\n",
        "    ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7GxPJ-yBSbE"
      },
      "source": [
        "Use these implementations below to implement the Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NrBfgSuBQ3o"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(LinearRegression):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the sigmoid function to the linear output\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the cross-entropy loss\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train!"
      ],
      "metadata": {
        "id": "wBS_tyOzfIED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkXUCz6zBxOn"
      },
      "outputs": [],
      "source": [
        "logistic_model = LogisticRegression(3072, 1)\n",
        "logi_val_accs = logistic_model.fit(X_train.double(), y_train, X_val.double(), y_val, 1e-4, 500000, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the validation accuracies of both models. Logistic Regression should be more accurate."
      ],
      "metadata": {
        "id": "pNzXSNN4fgOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28G1Z5_zCIva"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(lin_val_accs, label='LinearRegression')\n",
        "plt.plot(logi_val_accs, label='LogisticRegression')\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1F6flbHBMD"
      },
      "source": [
        "### Softmax Regression\n",
        "\n",
        "Softmax regression is similar to logistic regression except it uses the softmax function instead of sigmoid and uses negative log likelihood for the loss. Here's the formulas:\n",
        "\n",
        "Softmax:\n",
        "\n",
        "$$\\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$$\n",
        "\n",
        "where z is the output of the linear model, and K is the number of classes.\n",
        "\n",
        "Negative Log Likelihood:\n",
        "\n",
        "$$-{\\log(p(y))}$$\n",
        "\n",
        "where p(y) is the predicted probability of the data point belonging to the true class y.\n",
        "\n",
        "Implement them below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUNVJMx2LoTN"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    # Implement the softmax function\n",
        "    # x is the input data with shape (batch_size, input_size)\n",
        "    # Return the softmax output with shape (batch_size, output_size)\n",
        "    ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output\n",
        "\n",
        "def nll_loss(pred_probs, targets):\n",
        "    # Calculate the negative log likelihood loss\n",
        "    # pred_probs is the predicted probability distribution with shape (batch_size, output_size)\n",
        "    # targets is the ground-truth labels with shape (batch_size,)\n",
        "    # Return the negative log likelihood loss with shape (batch_size,)\n",
        "\n",
        "    # Hine: Convert the ground-truth labels to one-hot encoding using torch.eye()\n",
        "    ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use these implementations below to implement the Softmax Regression model."
      ],
      "metadata": {
        "id": "1-hnCw54gdDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzYXxe6TGTjP"
      },
      "outputs": [],
      "source": [
        "class SoftmaxRegression(LinearRegression):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SoftmaxRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the softmax function to the linear output\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the cross-entropy loss\n",
        "        ################# Your Implementations #################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # Calculate the predicted labels\n",
        "        y_pred = self.forward(x)\n",
        "        y_pred = y_pred.argmax(dim=0)\n",
        "        return (y_pred == y).float().mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcI45vFhMc32"
      },
      "outputs": [],
      "source": [
        "# Prepare multi-class dataset\n",
        "# Here \"ori\" indicates \"original\"\n",
        "X_train_ori_multi, y_train_multi = multi_train.dataset['data'], multi_train.dataset['label']\n",
        "X_val_ori_multi, y_val_multi = multi_val.dataset['data'], multi_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train_multi = normalize(X_train_ori_multi)\n",
        "X_val_multi = normalize(X_val_ori_multi)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train! Don't worry if your accuracy is low, that's expected."
      ],
      "metadata": {
        "id": "FsqHYABLgwQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBlVP4RIMQ4W"
      },
      "outputs": [],
      "source": [
        "sfm_model = SoftmaxRegression(3072, 20)\n",
        "sfm_val_accs = sfm_model.fit(X_train_multi.double(), y_train_multi, X_val_multi.double(), y_val_multi, 1e-4, 100000, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rndJ7OjaRHef"
      },
      "source": [
        "### Comparison\n",
        "\n",
        "In your own words, explain the differences between Linear Regression, Logistic Regression and Softmax Regression:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I12Gpv5nROiA"
      },
      "source": [
        "*TODO: Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "X6eGHF9mlQ-F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhyTcx3GTTgy"
      },
      "source": [
        "Now, we'll be using MiniPlaces, so there will be 100 different labels (instead of 20).\n",
        "\n",
        "You will use PyTorch to implement linear layers and ReLu to create a multi-layer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp7t5IhHRLlB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "        if split == \"train\" or split == \"val\":\n",
        "            with open(os.path.join(root_dir, (\"train\" if self.split == \"train\" else \"val\") + \".txt\")) as f:\n",
        "                for line in f:\n",
        "                    line = line.rstrip().split()\n",
        "                    n = int(line[0][-12:-4])\n",
        "                    if n <= 900:\n",
        "                        self.filenames.append(os.path.join(line[0]))\n",
        "                        self.labels.append(int(line[1]))\n",
        "        if label_dict is None and split == \"train\":\n",
        "            with open(os.path.join(root_dir, \"train.txt\")) as f:\n",
        "                num = -1\n",
        "                for line in f:\n",
        "                    line = line.rstrip().split()\n",
        "                    if int(line[1]) > num:\n",
        "                        num += 1\n",
        "                        self.label_dict.update({int(line[1]): line[0][8:line[0].find(\"/\", 8)]})\n",
        "        if split == \"test\":\n",
        "            self.labels = os.listdir(os.path.join(root_dir, \"images\", \"test\"))\n",
        "            self.filenames = [\"test/\" + i for i in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        dataset_len = len(self.labels)\n",
        "        return dataset_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.root_dir, \"images\", self.filenames[idx]))\n",
        "        if not self.transform is None:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "miniplaces_train = MiniPlaces(data_root, split='train', transform=data_transform)\n",
        "miniplaces_val = MiniPlaces(\n",
        "    data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=miniplaces_train.label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9ox6K8J0DrYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFDgb3kKTjat"
      },
      "outputs": [],
      "source": [
        "class FastMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize an MLP classifier.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: input_size, hidden_size\n",
        "          2) fc2: hidden_size, hidden_size\n",
        "          3) fc3: hidden_size, num_classes\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "            num_classes (int): Number of classes in the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the MLP\n",
        "        # Hint: The imported modules should give you a clue on which PyTorch functions to use\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP classifier.\n",
        "        Using ReLU as the activation function after each layer, except for the output layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the MLP classifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G8tCcrbVqKA"
      },
      "source": [
        "\n",
        "Then, define the training and evaluation functions to train and test the MLP classifier (You don't need to modify this part):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdZoW0woXU7M"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "    best_acc = 0\n",
        "    flag = False\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        if best_acc > accuracy:\n",
        "            if flag:\n",
        "                print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "                break\n",
        "            else:\n",
        "                flag = True\n",
        "        else:\n",
        "            best_acc = accuracy\n",
        "            flag = False\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def predict(model, test_dataloader):\n",
        "    \"\"\"\n",
        "    Evaluate the classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for i in test_dataloader:\n",
        "        pic = i[0]\n",
        "        lab = torch.argmax(model.to('cpu')(pic))\n",
        "        out.append(lab.item())\n",
        "\n",
        "    return out\n",
        "\n",
        "data_transform_flatten = transforms.Compose([data_transform, torch.flatten])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRE09jlZWU4f"
      },
      "outputs": [],
      "source": [
        "model = FastMLP(\n",
        "    input_size=3 * 64 * 64,\n",
        "    hidden_size=1024,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform_flatten)\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform_flatten,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "### !!! DON'T CHANGE NUM_WORKERS FROM 0 !!!\n",
        "### Using the loader will crash the notebook\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=64, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train!"
      ],
      "metadata": {
        "id": "ImnHQCy94d5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ],
      "metadata": {
        "id": "uPndWIt14Ga_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb34NMncz37N"
      },
      "source": [
        "Don't worry if your accuracy is low, that's to be expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z81jog_JhVg"
      },
      "source": [
        "## Q3: Convolutional Neural Network (CNN)\n",
        "\n",
        "This question is similar to the last, but you'll be implementing a convolutional neural network instead. You'll need to use convolutional layers, a max pooling layer, and dropout layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSck1VsjukiW"
      },
      "outputs": [],
      "source": [
        "class FastConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels, conv_hidden_channels, conv_out_channels,\n",
        "        input_size=(64,64),\n",
        "        dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "        fc_out_channels=128, num_classes=100,\n",
        "        kernel_size=3, stride=1, padding=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          input_channels (int): Number of channels in the input image.\n",
        "          conv_hidden_channels (int): Number of channels in the first convolutional layer.\n",
        "          conv_out_channels (int): Number of channels in the second convolutional layer.\n",
        "          input_size (tuple, optional): Height and width of the input image. (default: (64,64))\n",
        "          dropout_rate1, dropout_rate2 (float, optional): Dropout rate for\n",
        "              the first/second dropout layer. (default: 0.25, 0.5)\n",
        "          fc_out_channels (int, optional): Number of neurons in the first fully\n",
        "              connected layer. (default: 128)\n",
        "          num_classes (int, optional): Number of classes in the final output layer. (default: 100)\n",
        "          kernel_size, stride, padding (int, optional): Parameters of convolutional layers.\n",
        "\n",
        "        Initialize a convolutional neural network.\n",
        "        You can use Pytorch's built-in nn.Conv2d function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        3) max_pooling: (batch_size, conv_out_channels, H, W)\n",
        "        4) fc1: (batch_size, flatten_size) -> (batch_size, fc_out_channels)\n",
        "        5) fc2: (batch_size, fc_out_channels) -> (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # We've given you 'flatten_size', since we aren't expecting you to have this formula memorized :)\n",
        "        flatten_size = conv_out_channels * int(((input_size[0] + (2 * padding) - kernel_size) / kernel_size) + 1) * int(((input_size[1] + (2 * padding) - kernel_size) / kernel_size) + 1)\n",
        "\n",
        "        self.conv1 = None\n",
        "        self.conv2 = None\n",
        "        self.max_pooling = None\n",
        "        self.dropout1 = None\n",
        "        self.dropout2 = None\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the convolutional neural network\n",
        "        # replace \"None\"s with your implementations.\n",
        "        # All you need to do is to pass the input arguments to different constructors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x, return_intermediate=False):\n",
        "        \"\"\"\n",
        "        Forward pass of the convolutional neural network.\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) Apply relu.\n",
        "        3) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        4) max_pooling: Perform max pooling on the output from conv2\n",
        "        5) dropout1: Perform dropout on the output from max_pooling\n",
        "        6) Flatten the output from dropout1\n",
        "        7) fc1: Pass through a fully connected layer\n",
        "        8) dropout2: Perform dropout on the output from fc1\n",
        "        9) Apply relu.\n",
        "        10) fc2: Pass the output from the actiction layer to through a fully connected\n",
        "                layer to produce the final output\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the convolutional neural network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug3ZOJj-JpPV"
      },
      "outputs": [],
      "source": [
        "conv_train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "conv_val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform)\n",
        "conv_train_loader = torch.utils.data.DataLoader(\n",
        "    conv_train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
        "conv_val_loader = torch.utils.data.DataLoader(\n",
        "    conv_val_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
        "\n",
        "model = FastConv(\n",
        "    input_channels=3, conv_hidden_channels=64, conv_out_channels=128,\n",
        "    input_size=(64,64),\n",
        "    dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "    fc_out_channels=128,\n",
        "    kernel_size=3, stride=1, padding=1,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train!"
      ],
      "metadata": {
        "id": "6LWP2QbY9QdB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MafiXWN0EdYv"
      },
      "outputs": [],
      "source": [
        "train(model, conv_train_loader, conv_val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z69YoNYJ0zq"
      },
      "source": [
        "I can get ~20% accuracy after two epochs. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLNVqBYnKBWQ"
      },
      "source": [
        "## Q4: Your own model\n",
        "\n",
        "You will construct your own model using built-in convolutional layers. You can base your model on the provided `FastConv` class, or choose to modify the number of convolutional layers, feature size, learning rate, optimizer, and other parameters to suit your needs. You can also use any data transformations you'd like. You can even base your model off of an already existing architecture. The only restriction is that you can't blindly import an entire model at once; you should be creating every layer using PyTorch. Create your model and train using cells below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Use these cells to design and train your model"
      ],
      "metadata": {
        "id": "dB45TDIjTSYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Previous questions will be super useful"
      ],
      "metadata": {
        "id": "7HubBLpnTeFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15A5iODRTeQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've designed your model and trained it, it's time to test it's accuracy! Use the \"test\" images and the 'predict' function.\n",
        "\n",
        "NOTE: Make sure when constructing your DataLoader, you set 'shuffle' to 'False'. This is so we can match your model's predictions to test images to find how accurate its predictions are."
      ],
      "metadata": {
        "id": "y1Rb39nXAV9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w_D5j7ziGMk"
      },
      "outputs": [],
      "source": [
        "### Create your DataLoader and use predict to get your model's predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a list containing your predictions, you should create a file named \"{your_name}.json\", containing \"{id: predicted_label}\" pairs. A function to do this has been provided, just pass in the output of the predict function from the previous cell."
      ],
      "metadata": {
        "id": "JvEI1w8XBhtz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YkerT4DmxWd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def preds_to_json(preds):\n",
        "    json_dict = {}\n",
        "    for i, n in enumerate(test):\n",
        "        json_dict[str('0000000'+str(i+1))[-8:]] = n\n",
        "    with open(os.path.join(root_dir,'data.json'), 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "## 'test_preds' should be the output of 'predict' from the previous cell\n",
        "preds_to_json(test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq5gxCQPDMm7"
      },
      "source": [
        "### Model Architecture Explanation\n",
        "\n",
        "In the cell below, explain your model's architecture and how you came up with it. Feel free to include as many details as you want, including how you decided on a learning rate or data transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcrlmHv6DMm8"
      },
      "source": [
        "*TODO: Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You've Finished!!!\n",
        "\n",
        "Great job completing the assessment! The root directory for this notebook should now include the .json with your test predictions. In order to submit this assessment, all you have to do is email us this notebook as an .ipynb and the .json (our emails are at the top of the notebook).\n",
        "\n",
        "Thanks for taking the time to work through these problems! We look forward to reading through your application. You should expect to hear back from us a few days after October 10th."
      ],
      "metadata": {
        "id": "lalsUe1tET5U"
      }
    }
  ]
}